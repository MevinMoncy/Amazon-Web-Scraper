# Amazon Web Scraping Project üåêüì¶

## Overview üìö

This project demonstrates web scraping using Python's Selenium and BeautifulSoup libraries to extract product information from Amazon search results. The goal is to search for a specific product (e.g., "Protein shaker bottle") on Amazon's website, extract details such as product names, prices, ratings, review counts, and URLs, and save this information in a CSV file.

## Table of Contents üìã

- [Why I Did This Project?](#why-i-did-this-project-)
- [Lessons Learned](#lessons-learned-)
- [Prerequisites](#Prerequisites-)
- [Installation](#installation-)
- [Usage](#Usage-)

## Why I Did This Project? ü§î

In embarking on this project, I aimed to enhance my Python programming skills and delve into the world of web scraping. As a coding enthusiast, I recognized the potential in mastering web scraping, a skill that unlocks the ability to extract valuable insights from websites for various applications.

This project also provided the opportunity to explore two powerful libraries: Selenium and BeautifulSoup. Through these tools, I honed my ability to automate web interactions and parse HTML content, adding valuable skills to my toolkit.

The motivation behind this project was rooted in the practical challenge of navigating Amazon's vast array of products. The process of sifting through listings, comparing prices, reviews, and descriptions can be time-consuming. To address this, I developed a web scraper that compiles all these details into an organized CSV file. This file can be easily filtered and searched, streamlining the product discovery process.

## Lessons Learned üß†

Throughout the development of this project, I gained several valuable insights:

1. **Web Scraping Basics**: I learned the fundamentals of web scraping, including how to use libraries like BeautifulSoup and Selenium to extract data from websites.

2. **HTML Structure**: I learned to navigate HTML structures and use CSS selectors to target specific elements on a webpage for scraping.

3. **Selenium Usage**: I became familiar with the Selenium library and how to use it to automate browser actions like navigating to URLs, entering search terms, and retrieving page source code.

4. **Data Parsing and Cleaning**: I learned how to extract relevant information from HTML tags, clean and format the data, and store it in a structured format such as a CSV file.

5. **Error Handling**: I discovered the importance of error handling to gracefully handle exceptions that may arise during web scrapings, such as missing elements or unexpected data.

6. **Project Organization**: I practiced structuring the project into functions, using meaningful variable names, and writing clear comments and docstrings to improve code readability and maintainability.

7. **Version Control**: I experienced the benefits of version control by using Git to track changes, and manage the project's development lifecycle.

By completing this project, I've gained practical experience in web scraping techniques and learned how to extract valuable insights from websites using Python.

## Prerequisites üõ†Ô∏è

To run this project, you'll need:
- Python 3.x üêç
- ChromeDriver (for Selenium) üåê
- Required Python libraries: `selenium`, `beautifulsoup4` üìö


## Installation üöÄ

1. Clone the repository to your local machine:
 - git clone https://github.com/MevinMoncy/amazon-web-scraping.git
   
2. Download the appropriate ChromeDriver executable from the official website and place it in the project's directory.


## Usage üöÄ

1. **Clone the Repository:**
   
2. **Install Dependencies:**

Make sure you have Python 3.x installed. Then, install the required libraries using the following command:

- pip install selenium beautifulsoup4


3. **Download and Configure WebDriver:**

Download the appropriate WebDriver for your browser (e.g., Chrome WebDriver) and place it in the project directory. Update the `PATH` variable in the code with the path to your WebDriver.


4. **Run the Scraper:**

Modify the `main` function with your desired search term (e.g., "protein shaker bottle") to scrape. Then, run the script.

The script will initiate the browser, perform the search on Amazon, scrape the product details, and save them in a CSV file named `results1.csv`.


5. **Explore the CSV Data:**

Once the script completes, you'll find a `results1.csv` file in the project directory. Open this CSV file using a spreadsheet application to explore and filter the collected product information.


6. **Customization:**

Feel free to customize the script to scrape additional information or refine the scraping process according to your needs. You can adjust the data extraction logic and CSV output format in the `extract_record` function.


7. **Note:**

Please use web scraping responsibly and follow the terms of service of websites. Make sure your scraping activities are ethical! And do not overload the website's servers with excessive requests.






   
